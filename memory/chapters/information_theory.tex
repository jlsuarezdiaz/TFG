\chapter{Teoría de la información y divergencias} \label{chapter:information_theory}

En este capítulo se hará una breve introducción a la teoría de la información. La teoría de la información proporciona una base teórica para muchos campos de la ciencia e ingeniería, especialmente en la informática, las telecomunicaciones o las probabilidades. Dentro de la teoría de la información nos centraremos en el estudio de las divergencias. El concepto de divergencia, asociado a distribuciones de probabilidad, es similar al de distancia, estableciendo una medida para determinar la cercanía entre distribuciones. Dicha medida será de gran utilidad en los algoritmos que estudiaremos en el capítulo \ref{chapter:dml_algorithms}. También veremos cómo estas divergencias permiten tratar la distribución normal multivariante matricialmente, pudiendo así hacer uso de las teorías desarrolladas en el capítulo \ref{chapter:matrices}.

\section{Introducción}

La teoría de la información es una rama de las matemáticas y de la teoría de la computación, cuya finalidad es establecer una medida rigurosa para cuantificar la información y el desorden presente en un mensaje de comunicación. Fue desarrollada con el objetivo de encontrar límites en las operaciones de procesamiento de señales como la compresión, el almacenamiento o la comunicación. En la actualidad, sus aplicaciones se extienden a la mayoría de campos de la ciencia y la ingeniería.

El concepto de información puede resultar bastante abstracto, y difícil de modelar con una única definición. La teoría de la información define la entropía como primer concepto para modelar la información. La entropia se define para una distribución de probabilidad, y mide la cantidad de incertidumbre de la fuente de información asociada a dicha distribución. Puede entenderse también como la cantidad de información que contiene una variable aleatoria sobre sí misma. Formalmente, su definición es la que se muestra a continuación.

\begin{definition}
    Sea $(\Omega, \mathcal{A},P)$ un espacio de probabilidad, y $X \colon \Omega \to \R$ una variable aleatoria, discreta o continua, en dicho espacio. Supongamos que $p$ es la función masa de probabilidad asociada, si $X$ es discreta, o la función de densidad, si $X$ es continua. Se define la \emph{entropía} asociada a la variable aleatoria $X$ como
    \[ H(x) = \mathbb{E}[1/\log(p(X))] = \mathbb{E}[-\log p(X) ] = - \int_{\Omega} \log(p(X(\omega)))\  d P, \]
    siempre que dicha integral exista. En particular, si $X$ es discreta y su función masa de probabilidad viene dada por $p(x) = P[X=x]$, entonces se tiene que
    \[ H(x) = -\sum_{x \in X(\Omega)} p(x)\log(p(x)) \]
    Si $X$ es una variable aleatoria continua, entonces la entropía de $X$ puede expresarse, siempre que el valor absoluto asociado tenga integral finita, como
    \[ H(x) = -\int_{-\infty}^{+\infty} p(x)\log(p(x))\ dx \]
    Por razones de continuidad, se asume que $0 \log 0 = 0$.
\end{definition}

Es inmediato comprobar que $H$ toma valores no negativos. También, si cambiamos la base del logaritmo, la entropía difiere únicamente en una constante, por lo que en muchos casos, según el campo de aplicación, se trabaja con una determinada base en la definición de entropía. Si se elige base $2$, la entropía se mide en \emph{bits}. A lo largo del capítulo se trabajará en base en $e$. En este caso, la entropía se mide en \emph{nats}.

La entropía mide la cantidad de incertidumbre o información esperada en un suceso. Por ejemplo, si una variable aleatoria toma dos valores con probabilidades $p$ y $1-p$, la entropía será máxima para $p = 1/2$, y será nula cuando $p \in \{0,1\}$. También, en base 2, es posible ver la entropía, sobre una variable discreta, como una aproximación del número esperado de preguntas binarias de la forma ``¿$X=x_i$?'' necesarias para acertar el valor que ha tomado $X$. Por tanto, si $X$ tiene una distribución parecida a la uniforme, su entropía será mayor frente a otras distribuciones más determinísticas.

Asociados a la entropía se definen numerosos conceptos, como la información mutua, que mide la cantidad de información que contiene una variable aleatoria sobre otra variable, o la entropía relativa, que es una forma de medir la cercanía entre distintas variables aleatorias. Nos centraremos en esta última, y en conceptos derivados de ella. Para ello, en primer lugar, definiremos el concepto de divergencia. La divergencia es una magnitud para medir la cercanía entre determinados objetos sobre un conjunto. No hablaremos de distancias, pues las magnitudes que consideraremos pueden no verificar algunas de las propiedades que se exigen a las distancias, como es el caso de la simetría, o la desigualdad triangular.


\begin{definition}
    Sea $X$ un conjunto. Una aplicación $D(\cdot \| \cdot) \colon X \times X \to \R$ se dice que es una \emph{divergencia} si satisface las siguientes propiedades:

    \begin{enumerate}
        \item No negatividad: $D(x\|y) \ge 0$ para todos $x, y \in X$.
        \item Coincidencia: $D(x\|y) = 0$ si y solo si $x = y$.
    \end{enumerate}    
\end{definition}


\section{Las divergencias de Kullback-Leibler y Jeffrey}

En esta sección presentaremos las divergencias asociadas a la entropía con las que vamos a trabajar.

\begin{definition}
    Sea $(\Omega, \mathcal{A},P)$ un espacio de probabilidad, y $X \colon \Omega \to \R$ una variable aleatoria, discreta o continua, en dicho espacio. Supongamos que $p$ es la función masa de probabilidad asociada, si $X$ es discreta, o la función de densidad, si $X$ es continua. Supongamos que $q$ es otra función masa de probabilidad o función de densidad. Entonces, se define la \emph{entropía relativa} o \emph{divergencia de Kullback-Leibler} entre $p$ y $q$, como
    \[ \kl(p \| q) = \mathbb{E}_p\left[\log\frac{p(X)}{q(X)}\right] = \int_{\Omega} \log\frac{p(X(\omega))}{q(X(\omega))} \ dP, \]
    siempre que dicha integral exista. En particular, si $p$ y $q$ son discretas y toman valores en los mismos puntos, se tiene que
    \[ \kl(p \| q) = \sum_{x \in X(\Omega)} p(x) \log\frac{p(x)}{q(x)}, \]
    y si $p$ y $q$ son continuas, siempre que el valor absoluto tenga integral finita, se tiene que
    \[ \kl(p \| q) = \int_{-\infty}^{+\infty} p(x) \log\frac{p(x)}{q(x)} \ dx. \]
    De nuevo por razones de continuidad, se asume que $0 \log (0/0) = 0$.
\end{definition}

Mediante la entropía relativa se mide la ineficiencia de asumir que $X$ sigue una distribución $q$ cuando la distribución real es $p$. Veamos que, efectivamente, esta aplicación es una divergencia (en el caso continuo, entenderemos la igualdad como igualdad c.p.d.).

\begin{thm}[Desigualdad de la información] \label{thm:desig_infor}
    La divergencia de Kullback-Leibler es una divergencia, es decir, $\kl(p \| q) \ge 0$ y se da la igualdad si y solo si $p(x) = q(x)$ c.p.d. en $X(\Omega)$ (la igualdad es en todo punto en el caso discreto).
\end{thm}

\begin{proof}
    Como la función $-\log$ es estrictamente convexa, podemos aplicar la desigualdad de Jensen \ref{thm:desig_jensen}, obteniendo
    \begin{align*}
        \kl(p \| q) &= \mathbb{E}_p\left[\log\frac{p(X)}{q(X)}\right] 
                    = \mathbb{E}_p\left[-\log\frac{q(X)}{p(X)}\right] \\
                    &\ge -\log\mathbb{E}_p\left[\frac{q(X)}{p(X)}\right] 
                    = -\log\int p(x) \frac{q(x)}{p(x)} \ dx \\
                    &= -\log\int q(x) \ dx = -\log 1 = 0.
    \end{align*}
    Aunque se ha usado la notación continua, el caso discreto es análogo. Además, la convexidad estricta implica que se da la igualdad si y solo si $p/q$ es constante c.p.d. si y solo si $p = q$ c.p.d., por ser funciones de densidad o masa de probabilidad. Además, como en el caso discreto $p$ y $q$ toman valores en conjuntos con probabilidad no nula, se tiene la igualdad en todo punto.
\end{proof}

Tenemos por tanto que, mediante la divergencia de Kullback-Leibler podemos medir la ``cercanía'' entre dos distribuciones de probabilidad, por lo que podrán ser utilizadas más adelante para acercar distribuciones convenientemente parametrizadas. En cambio, no es tan útil para medir la ``lejanía'' entre distribuciones, puesto que al no ser simétrica, los valores de $\kl(p \| q)$ y $\kl(q \| p)$ pueden ser muy diferentes cuando $p$ y $q$ no se parecen. Por eso, a veces es útil trabajar con una divergencia de Kullback-Leibler simetrizada. Esta divergencia se conoce como divergencia de Jeffrey.

\begin{definition}
    Se define la divergencia de Jeffrey para dos distribuciones $p$ y $q$ para las que existan $\kl(p \| q)$ y $\kl(q \| p)$ como
    \[ \jf(p\|q) = \kl(p \| q) + \kl(q \| p). \]
    En el caso discreto, se tiene que
    \[ \jf(p\| q) = \sum_{i =1}^N (p(x_i)-q(x_i))(\log p(x_i) - \log q(x_i)).\]
    Para el caso continuo, se verifica que
    \[ \jf(p\|q) = \int_{-\infty}^{+\infty} (p(x) - q(x))(\log p(x) - \log q(x)) \ dx. \]
\end{definition}

Por la desigualdad de la información es inmediato ver que la divergencia de Jeffrey es efectivamente una divergencia, y además, $J(p\|q) = J(q\|p)$. Observemos que ambas divergencias son funciones de las distribuciones de probabilidad, dependiendo únicamente de las probabilidades establecidas en ellas, y no de los valores que tomen las correspondientes variables aleatorias. Esto permite extender las divergencias a vectores aleatorios, siempre que conozcamos sus funciones de densidad o masa de probabilidad.

\section{La distribución normal multivariante y divergencias matriciales.}

Para concluir la sección, vamos a estudiar las aplicaciones de las divergencias de Kullback-Leibler y de Jeffrey sobre la familia de distribuciones gaussianas multivariante. Estas distribuciones son la generalización natural de la distribución normal de una variable. Sea $\mu \in \R^d$ y $\Sigma \in \mathcal{M}_d(\R)^+$ una matriz definida positiva. Se dice que un vector aleatorio $X = (X_1,\dots,X_d)$ sigue una distribución normal multivariante con media $\mu$ y matriz de covarianza $\Sigma$, si tiene una función de densidad asociada
\[ p(x|\mu,\Sigma) = \frac{1}{(2\pi)^{d/2} \det(\Sigma)^{1/2}} \exp\left(-\frac{1}{2} (x-\mu)^T\Sigma^{-1}(x-\mu)\right). \]
Se verifica que $\mathbb{E}[X] = \mu$ y $\cov(X) = \mathbb{E}[(X-\mathbb{E}[X])(X - \mathbb{E}[X])^T] = \Sigma$, y por tanto las distribuciones gaussianas quedan determinadas por su media y su covarianza. Es importante recordar que, si $L \in \mathcal{M}_d(\R)$ es una aplicación lineal, entonces, el vector aleatorio $LX$ sigue una distribución normal multivariante de media $L\mu$ y covarianza $L\Sigma L^T$.

Queremos establecer, para distribuciones normales multivariante, correspondencias entre las divergencias estudiadas y divergencias matriciales. Las divergencias matriciales proporcionan una alternativa para medir la cercanía entre matrices a la ya estudiada en el capítulo \ref{chapter:matrices} con la norma de Frobenius. Nos centraremos en las conocidas como divergencias de Bregman.

\begin{definition}
    Sea $K \subset \mathcal{M}_d(\R)$ un conjunto convexo y abierto, y $\phi \colon K \to \R$ una función estrictamente convexa y diferenciable. Se define la \emph{divergencia de Bregman} asociada a $\phi$ como la aplicación $D_{\phi}(\cdot \| \cdot) \colon K \times K \to \R$ dada por
    \[ D_{\phi}(A\|B) = \phi(A) - \phi(B) - \tr(\nabla\phi(B)^T(A-B)) = \phi(A) - \phi(B) - \langle \nabla\phi(B), B-A\rangle_F.\]
\end{definition}

Es interesante destacar que para $\phi(X) = \|X\|_F^2$, la divergencia de Bregman asociada es $D_{\phi}(A\|B) = \|A-B\|_F^2$, es decir, la distancia asociada a la norma de Frobenius estudiada en el capítulo anterior. Es claro que la divergencia de Bregman es una divergencia, para cualquier $\phi$ en las condiciones de la definición, gracias al apartado \ref{item:prop:convex_functions:gradient} de la proposición \ref{prop:convex_functions_opt_dif}.

De todas las posibles divergencias de Bregman, nos centraremos en aquella asociada a la función \emph{log-det}, es decir, la función $\phi_{ld} \colon \mathcal{M}_d(\R)^+ \to \R$ dada por
\[ \phi_{ld}(M) = - \log \det(M) = -\log\prod_{i=1}^d \lambda_i = -\sum_{i=1}^d \log\lambda_i, \]
donde $0 < \lambda_1 \le \dots \le \lambda_d$ son los valores propios de $M$. 

\begin{prop}
    $\phi_{ld}$ es estrictamente convexa en $\mathcal{M}_d(\R)^+$.
\end{prop}

\begin{proof}
    Fijadas $M_0, M \in \mathcal{M}_d(\R)^+$, definimos $g\colon [0,1] \to \R$ por $g(t) = -\phi_{ld}(M_0 + tM) = \log\det(M_0 + tM)$. Extrayendo raíces cuadradas a izquierda y derecha, y usando las propiedades del determinante y logaritmo, obtenemos
    \begin{align*}
        \log\det(M_0 + tM) &= \log\det(M_0^{1/2}(I + tM_0^{1/2}MM_0^{1/2})M_0^{1/2}) \\
                           &= \log(\det(I+ tM_0^{1/2}MM_0^{1/2})\det(M_0^{1/2})\det(M_0^{1/2}) \\
                           &= \sum_{i=1}^d\log(1+t\sigma_i) + \log\det(M_0),
    \end{align*}
    donde $\sigma_1 \le \dots \le \sigma_d$ son los valores propios de $N = M_0^{1/2}MM_0^{1/2}$, los cuales son todos positivos pues es una $N$ y $M$ son congruentes, $M$ es definida positiva, y la congruencia preserva el signo de los valores propios. Entonces,
    \[g'(t) = \sum_{i=1}^d \frac{\sigma_i}{1+t\sigma_i} \quad g''(t) = - \sum_{i=1}^d \frac{\sigma_i^2}{(1+t\sigma_i)^2} < 0 \quad \forall t \in [0,1].   \]
    Por tanto, $g$ es estrictamente cóncava, luego $-\phi_{ld}$ es estrictamente cóncava y $\phi_{ld}$ es estrictamente convexa.
\end{proof}

Como consecuencia de la proposición, $\phi_{ld}$ determina una divergencia de Bregman, la cual se denomina divergencia \emph{log-det} y se nota $D_{ld}$. Podemos calcular su expresión, para $A, B \in \mathcal{M}_d(\R)^+$, obteniendo
\[ D_{ld}(A\|B) = \log\det(B) - \log\det(A) + \langle B^{-1},A-B\rangle_F = \tr(AB^{-1}) -\log\det(AB^{-1}) -d.\]
Si llamamos $a_1,\dots,a_d$ a los vectores propios de $A$, con correspondientes valores propios $\lambda_1, \dots,\lambda_d$ y llamamos $b_1,\dots,b_d$ a los vectores propios de $B$ con valores propios $\mu_1,\dots,\mu_d$, se tiene entonces la expresión
\[ D_{ld}(A\|B) = \sum_{i=1}^d\sum_{j=1}^d \frac{\lambda_i}{\mu_j}\langle a_i,b_j\rangle^2 - \sum_{i=1}^d \log \frac{\lambda_i}{\mu_i} - d.\]

Vamos finalmente a calcular la entropía relativa entre dos distribuciones normales. Sean $\mu_1,\mu_2 \in \R^d$ y $\Sigma_1,\Sigma_2 \in \mathcal{M}_d(\R)^+$. Consideramos las distribuciones gaussianas asociadas a $(\mu_1,\Sigma_1)$ y $(\mu_2,\Sigma_2)$, respectivamente, determinadas por sus funciones de densidad $p(x|\mu_1,\Sigma_1)$ y $p(x|\mu_2,\Sigma_2)$. Buscamos calcular $\kl(p(x|\mu_1,\Sigma_1)\|p(x|\mu_2,\Sigma_2))$. Si abreviamos las distribuciones por $p_1$ y $p_2$, respectivamente, tenemos, en primer lugar, que
\[ \kl(p_1\|p_2) = \int p_1(x)\log\frac{p_1(x)}{p_2(x)}\ dx = \int p_1(x) \log p_1(x) - \int p_1(x) \log p_2(x) = -H(p_1) - \int p_1 \log p_2(x). \]
Podemos calcular la entropía asociada a $p_1$, como
\begin{equation} \label{eq:kl_gaussian:sum_1}
\begin{split}
 H(p_1) &= - \int p_1(x) \left[ \frac{-1}{2}(x-\mu_1)^T\Sigma_1^{-1}(x-\mu_1) - \log((2\pi)^{d/2}\det(\Sigma_1)^{1/2}) \right] \ dx \\
        &= \frac{1}{2} \mathbb{E}_{p_1}\left[ \sum_{i,j=1}^d (x_i - \mu_i^{(1)}) (\Sigma_1^{-1})_{ij}(x_j - \mu_j^{(1)})\right] + \frac{1}{2}\log((2\pi)^d\det(\Sigma_1)) \\
        &= \frac{1}{2} \sum_{i,j=1}^d\mathbb{E}_{p_1}[(x_j - \mu_j^{(1)})(x_i - \mu_i^{(1)})](\Sigma_1^{-1})_{ij} + \frac{1}{2}\log((2\pi)^d\det(\Sigma_1)) \\
        &= \frac{1}{2} \sum_{j=1}^d\sum_{i=1}^d (\Sigma_1)_{ji}(\Sigma_1^{-1})_{ij} + \frac{1}{2}\log((2\pi)^d\det(\Sigma_1)) \\
        &= \frac{1}{2} \sum_{j=1}^d (\Sigma_1\Sigma_1^{-1})_{jj} + \frac{1}{2}\log((2\pi)^d\det(\Sigma_1)) \\
        &= \frac{1}{2} \sum_{j=1}^d 1 + \frac{1}{2}\log((2\pi)^d\det(\Sigma_1)) \\
        &= \frac{d}{2} + \frac{1}{2}\log((2\pi)^d\det(\Sigma_1)),
\end{split}
\end{equation}
donde para el cuarto paso se ha utilizado la definición de matriz de covarianza, y para el quinto paso se ha utilizado la definición elemento a elemento del producto de matrices. Calculamos ahora el segundo sumando de la expresión de la divergencia.
\begin{equation} \label{eq:kl_gaussian:sum_2}
\begin{split}
    \int p_1(x)\log p_2(x) &= \int p_1(x) \left[-\frac{1}{2}(x-\mu_2)^T\Sigma_2^{-1}(x-\mu_2) - \log((2\pi)^{d/2}\det(\Sigma_2)^{1/2}) \right] \ dx\\
                  &= -\frac{1}{2} \sum_{i,j=1}^d(\Sigma_2^{-1})_{ij}\mathbb{E}_{p_1}[(x_j - \mu_j^{(2)})(x_i - \mu_i^{(2)})] - \frac{1}{2}\log((2\pi)^d\det(\Sigma_2)) \\
                  &= -\frac{1}{2} \tr(\Sigma_2^{-1}\mathbb{E}_{p_1}[(x-\mu_2)(x-\mu_2)^T])- \frac{1}{2}\log((2\pi)^d\det(\Sigma_2)) \\
                  &= -\frac{1}{2} \tr(\Sigma_2^{-1}\mathbb{E}_{p_1}[((x-\mu_1)-(\mu_1 - \mu_2))((x-\mu_1)-(\mu_1 - \mu_2))^T]) - \frac{1}{2}\log((2\pi)^d\det(\Sigma_2)) \\
                  &= -\frac{1}{2} \tr(\Sigma_2^{-1}\Sigma_1 + \Sigma_2^{-1}(\mu_1-\mu_2)(\mu_1-\mu_2)^T) - \frac{1}{2}\log((2\pi)^d\det(\Sigma_2)) \\
                  &= -\frac{1}{2} \tr(\Sigma_2^{-1}\Sigma_1) - \frac{1}{2}(\mu_1-\mu_2)^T\Sigma_2^{-1}(\mu_1 - \mu_2) - \frac{1}{2}\log((2\pi)^d\det(\Sigma_2)),
\end{split}
\end{equation}
donde en el quinto paso se ha utilizado, de nuevo, la definición de matriz de covarianza, y que $\mathbb{E}_{p_1}[(x-\mu_1)(\mu_1-\mu_2)^T] = \mathbb{E}_{p_1}[x-\mu_1](\mu_1-\mu_2)^T = 0$. Combinando las expresiones obtenidas en \ref{eq:kl_gaussian:sum_1} y \ref{eq:kl_gaussian:sum_2}, obtenemos
\begin{equation}
\begin{split}
 \kl(p_1\|p_2) &= -\frac{d}{2} -\frac{1}{2}\log((2\pi)^d\det(\Sigma_1))+\frac{1}{2} \tr(\Sigma_2^{-1}\Sigma_1)+\frac{1}{2}(\mu_1-\mu_2)^T\Sigma_2^{-1}(\mu_1 - \mu_2)+\frac{1}{2}\log((2\pi)^d\det(\Sigma_2))\\
    &= \frac{1}{2}\left( \tr(\Sigma_1\Sigma_2^{-1}) - \log\det(\Sigma_1\Sigma_2^{-1}) -d \right) + \frac{1}{2}(\mu_1-\mu_2)^T\Sigma_2^{-1}(\mu_1-\mu_2) \\
    &= \frac{1}{2}D_{ld}(\Sigma_1\|\Sigma_2) + \frac{1}{2}\|\mu_1-\mu_2\|_{\Sigma_2^{-1}}^2.
\end{split}
\end{equation}
Acabamos de obtener el siguiente resultado.
\begin{thm} \label{prop:caract_kl:1}
    La divergencia de Kullback-Leibler entre dos distribuciones gaussianas multivariante definidas por funciones de densidad $p_1(x|\mu_1,\Sigma_1)$ y $p_2(x|\mu_2,\Sigma_2)$, con $\mu_1,\mu_2 \in \R^d$ y $\Sigma_1,\Sigma_2 \in \mathcal{M}_d(\R)^+$, verifica
    \[  \kl(p_1\|p_2) = \frac{1}{2}D_{ld}(\Sigma_1\|\Sigma_2) + \frac{1}{2}\|\mu_1-\mu_2\|_{\Sigma_2^{-1}}^2. \]
\end{thm}
\begin{cor} \label{prop:caract_kl:2}
    La divergencia de Kullback-Leibler entre dos distribuciones gaussianas multivariante definidas por funciones de densidad $p_1$ y $p_2$ con igual media, y covarianzas $\Sigma_1$ y $\Sigma_2$, verifica
    \[  \kl(p_1\|p_2) = \frac{1}{2}D_{ld}(\Sigma_1\|\Sigma_2). \]
\end{cor}
De forma inmediata se obtienen expresiones para la divergencia de Jeffrey.
\begin{cor} \label{prop:caract_jeffrey:1}
    La divergencia de Jeffrey entre dos distribuciones gaussianas multivariante definidas por funciones de densidad $p_1(x|\mu_1,\Sigma_1)$ y $p_2(x|\mu_2,\Sigma_2)$, con $\mu_1,\mu_2 \in \R^d$ y $\Sigma_1,\Sigma_2 \in \mathcal{M}_d(\R)^+$, verifica
    \[  \jf(p_1\|p_2) = \frac{1}{2}\tr(\Sigma_1\Sigma_2^{-1}+\Sigma_1^{-1}\Sigma_2) -d + \frac{1}{2}\|\mu_1-\mu_2\|_{\Sigma_1^{-1}+\Sigma_2^{-1}}^2. \]
\end{cor}
\begin{proof}
    \begin{equation*}
        \begin{split}
        \jf(p_1\|p_2) &= \kl(p_1\|p_2) + \kl(p_2\|p_1) \\
                    &= \frac{1}{2}D_{ld}(\Sigma_1\|\Sigma_2) + \frac{1}{2}(\mu_1-\mu_2)^T\Sigma_2^{-1}(\mu_1-\mu_2) + \frac{1}{2}D_{ld}(\Sigma_2\|\Sigma_1) + (\mu_1-\mu_2)^T\Sigma_1^{-1}(\mu_1-\mu_2) \\
                    &= \frac{1}{2}\left[\tr(\Sigma_1\Sigma_2^{-1})+\tr(\Sigma_1^{-1}\Sigma_2)\right] - \frac{1}{2}\left[\log\det(\Sigma_1\Sigma_2^{-1})+\log\det(\Sigma_1^{-1}\Sigma_2)\right]\\&+\frac{1}{2}(\mu_1-\mu_2)^T(\Sigma_1^{-1}+\Sigma_2^{-1})(\mu_1-\mu_2) - \frac{1}{2}(d+d) \\
                    &= \frac{1}{2} \tr(\Sigma_1\Sigma_2^{-1}+\Sigma_1^{-1}\Sigma_2) -d + \frac{1}{2}\|\mu_1-\mu_2\|_{\Sigma_1^{-1}+\Sigma_2^{-1}}^2 - \frac{1}{2}(\log\det\Sigma_1-\log\det\Sigma_2+\log\det\Sigma_2-\log\det\Sigma_1) \\
                    &= \frac{1}{2}\tr(\Sigma_1\Sigma_2^{-1}+\Sigma_1^{-1}\Sigma_2) -d + \frac{1}{2}\|\mu_1-\mu_2\|_{\Sigma_1^{-1}+\Sigma_2^{-1}}^2.
        \end{split}
    \end{equation*}
\end{proof}
\begin{cor} \label{prop:caract_jeffrey:2}
    La divergencia de Jeffrey entre dos distribuciones gaussianas multivariante definidas por funciones de densidad $p_1$ y $p_2$ con igual media, y covarianzas $\Sigma_1$ y $\Sigma_2$, verifica
    \[  \jf(p_1\|p_2) = \frac{1}{2}\tr(\Sigma_1\Sigma_2^{-1}+\Sigma_1^{-1}\Sigma_2) -d. \]
\end{cor}