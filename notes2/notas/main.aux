\relax 
\providecommand\hyper@newdestlabel[2]{}
\catcode `"\active 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@sortscheme{nty}
\abx@aux@sortnamekeyscheme{global}
\select@language{spanish}
\@writefile{toc}{\select@language{spanish}}
\@writefile{lof}{\select@language{spanish}}
\@writefile{lot}{\select@language{spanish}}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\select@language{spanish}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\select@language{spanish}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\select@language{spanish}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\select@language{spanish}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Matem\IeC {\'a}ticas}{3}{part.1}}
\newlabel{prop:caract_distancias}{{0.0.1}{5}{Caracterizaciones de distancias asociadas a métricas}{thm.0.0.1}{}}
\newlabel{thm:eigen_trace_opt}{{0.0.2}{5}{Optimización de la traza por vectores propios}{thm.0.0.2}{}}
\newlabel{thm:eigen_trace_ratio_opt}{{0.0.3}{5}{}{thm.0.0.3}{}}
\newlabel{thm:iter_proj}{{0.0.4}{5}{Convergencia de las proyecciones iteradas}{thm.0.0.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {part}{II\hspace  {1em}Inform\IeC {\'a}tica te\IeC {\'o}rica}{7}{part.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {1}El aprendizaje autom\IeC {\'a}tico}{9}{chapter.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introducci\IeC {\'o}n}{9}{section.1.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.2}El aprendizaje supervisado}{9}{section.1.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.3}El problema de clasificaci\IeC {\'o}n}{9}{section.1.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {2}El aprendizaje de m\IeC {\'e}tricas de distancia}{11}{chapter.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.1}Distancias. Distancias en espacios de Hilbert.}{11}{section.2.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.2}Descripci\IeC {\'o}n del problema}{11}{section.2.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.3}Aplicaciones}{11}{section.2.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.4}El aprendizaje por semejanza}{11}{section.2.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {3}Descripci\IeC {\'o}n te\IeC {\'o}rica de t\IeC {\'e}cnicas de aprendizaje de m\IeC {\'e}tricas de distancia}{13}{chapter.3}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.1}T\IeC {\'e}cnicas de reducci\IeC {\'o}n de dimensionalidad}{13}{section.3.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}PCA}{13}{subsection.3.1.1}}
\newlabel{eq:pca:compress}{{3.1}{14}{PCA}{equation.3.1.1}{}}
\newlabel{eq:pca:compress2}{{3.2}{15}{PCA}{equation.3.1.2}{}}
\newlabel{eq:pca:traceproblem}{{3.3}{15}{PCA}{equation.3.1.3}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Ejemplificaci\IeC {\'o}n gr\IeC {\'a}fica del PCA. En la primera imagen se muestra un conjunto de datos, junto con las direcciones principales (proporcionales de acuerdo a la varianza explicada) aprendidas por PCA. A su derecha, los datos proyectados en dimensi\IeC {\'o}n m\IeC {\'a}xima. Observamos que dicha proyecci\IeC {\'o}n consiste en girar los datos haciendo coincidir los ejes con las direcciones principales. Abajo a la izquierda, los datos proyectados sobre la primera componente principal. Por \IeC {\'u}ltimo, a su derecha, los datos recuperados mediante la matriz de descompresi\IeC {\'o}n, junto con los datos originales. Podemos comprobar que la proyecci\IeC {\'o}n de PCA es la que minimiza el error cuadr\IeC {\'a}tico de descompresi\IeC {\'o}n. En este caso particular los datos descomprimidos se encuentran en la recta de regresi\IeC {\'o}n de los datos originales, debido a las dimensiones del problema.\relax }}{16}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:pca}{{3.1}{16}{Ejemplificación gráfica del PCA. En la primera imagen se muestra un conjunto de datos, junto con las direcciones principales (proporcionales de acuerdo a la varianza explicada) aprendidas por PCA. A su derecha, los datos proyectados en dimensión máxima. Observamos que dicha proyección consiste en girar los datos haciendo coincidir los ejes con las direcciones principales. Abajo a la izquierda, los datos proyectados sobre la primera componente principal. Por último, a su derecha, los datos recuperados mediante la matriz de descompresión, junto con los datos originales. Podemos comprobar que la proyección de PCA es la que minimiza el error cuadrático de descompresión. En este caso particular los datos descomprimidos se encuentran en la recta de regresión de los datos originales, debido a las dimensiones del problema.\relax }{figure.caption.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}LDA}{17}{subsection.3.1.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Ejemplo gr\IeC {\'a}fico del LDA y comparaci\IeC {\'o}n con PCA. En la primera imagen se muestra un conjunto de datos, con la direcci\IeC {\'o}n principal determinada por PCA, en naranja, y la direcci\IeC {\'o}n determinada por LDA, en verde. Observamos que si proyectamos los datos sobre la direcci\IeC {\'o}n obtenida por LDA quedan bien separados, como se muestra en la imagen derecha. En cambio, la direcci\IeC {\'o}n obtenida por PCA solo nos permite maximizar la varianza de todo el conjunto al proyectar, pues no considera la informaci\IeC {\'o}n de las etiquetas.\relax }}{17}{figure.caption.3}}
\newlabel{fig:lda}{{3.2}{17}{Ejemplo gráfico del LDA y comparación con PCA. En la primera imagen se muestra un conjunto de datos, con la dirección principal determinada por PCA, en naranja, y la dirección determinada por LDA, en verde. Observamos que si proyectamos los datos sobre la dirección obtenida por LDA quedan bien separados, como se muestra en la imagen derecha. En cambio, la dirección obtenida por PCA solo nos permite maximizar la varianza de todo el conjunto al proyectar, pues no considera la información de las etiquetas.\relax }{figure.caption.3}{}}
\abx@aux@cite{maulik2002performance}
\newlabel{eq:lda}{{3.6}{18}{LDA}{equation.3.1.6}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}ANMM}{19}{subsection.3.1.3}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Descripci\IeC {\'o}n gr\IeC {\'a}fica del margen promedio de vecindario para el dato $x_i$, para $\xi $ = $\zeta $ = 3. Las circunferencias azul y roja determinan la distancia media de $x_i$ a los datos de igual y distinta clase, respectivamente.\relax }}{20}{figure.caption.4}}
\newlabel{fig:average_neighbor_margin}{{3.3}{20}{Descripción gráfica del margen promedio de vecindario para el dato $x_i$, para $\xi $ = $\zeta $ = 3. Las circunferencias azul y roja determinan la distancia media de $x_i$ a los datos de igual y distinta clase, respectivamente.\relax }{figure.caption.4}{}}
\newlabel{eq:margin_caract}{{3.9}{21}{ANMM}{equation.3.1.9}{}}
\abx@aux@cite{lmnn}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.2}T\IeC {\'e}cnicas orientadas a la mejora del clasificador de vecinos cercanos}{22}{section.3.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}LMNN}{22}{subsection.3.2.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Descripci\IeC {\'o}n gr\IeC {\'a}fica de vecinos objetivo e impostores (para $k = 3$) para el dato $x_i$. El c\IeC {\'\i }rculo azul representa el margen que determinan los vecinos objetivo. Todos los puntos de distintas clases en dicho c\IeC {\'\i }rculo son impostores. El objetivo LMNN ser\IeC {\'a} acercar los vecinos objetivos lo m\IeC {\'a}ximo posible y eliminar los impostores del c\IeC {\'\i }rculo. Por tanto, no influir\IeC {\'a}n los datos de la misma clase que no sean vecinos objetivo y se dejar\IeC {\'a}n de penalizar los impostores en cuanto salgan del margen, como se muestra a la derecha. Esto da un car\IeC {\'a}cter local a esta t\IeC {\'e}cnica de aprendizaje.\relax }}{23}{figure.caption.5}}
\newlabel{fig:targets_impostors}{{3.4}{23}{Descripción gráfica de vecinos objetivo e impostores (para $k = 3$) para el dato $x_i$. El círculo azul representa el margen que determinan los vecinos objetivo. Todos los puntos de distintas clases en dicho círculo son impostores. El objetivo LMNN será acercar los vecinos objetivos lo máximo posible y eliminar los impostores del círculo. Por tanto, no influirán los datos de la misma clase que no sean vecinos objetivo y se dejarán de penalizar los impostores en cuanto salgan del margen, como se muestra a la derecha. Esto da un carácter local a esta técnica de aprendizaje.\relax }{figure.caption.5}{}}
\newlabel{eq:lmnn:L}{{3.10}{23}{LMNN}{equation.3.2.10}{}}
\newlabel{eq:lmnn:M}{{3.11}{24}{LMNN}{equation.3.2.11}{}}
\abx@aux@cite{nca}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}NCA}{25}{subsection.3.2.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{El kNN y la validaci\IeC {\'o}n \emph  {Leave One Out}}{25}{section*.6}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{El an\IeC {\'a}lisis de componentes de vecindarios}{26}{section*.7}}
\abx@aux@cite{ncmml}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.3}T\IeC {\'e}cnicas orientadas a la mejora del clasificador de centroides cercanos}{27}{section.3.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}NCMML}{27}{subsection.3.3.1}}
\abx@aux@cite{clustering_algorithms}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}NCMC}{28}{subsection.3.3.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{Generalizando NCM: El clasificador de m\IeC {\'u}ltiples centroides}{28}{section*.8}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Conjunto de datos donde el clasificador NCM no da buenos resultados, pues los centroides de ambas clases son muy cercanos y ambos caen entre los puntos de la clase 1. Veremos que, escogiendo m\IeC {\'a}s de un centroide de forma adecuada podremos clasificar este conjunto como se muestra en la figura de la derecha.\relax }}{28}{figure.caption.9}}
\newlabel{fig:problema_ncm}{{3.5}{28}{Conjunto de datos donde el clasificador NCM no da buenos resultados, pues los centroides de ambas clases son muy cercanos y ambos caen entre los puntos de la clase 1. Veremos que, escogiendo más de un centroide de forma adecuada podremos clasificar este conjunto como se muestra en la figura de la derecha.\relax }{figure.caption.9}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{K-Means y la b\IeC {\'u}squeda de centroides}{29}{section*.10}}
\newlabel{eq:obj:kmeans}{{3.20}{29}{K-Means y la búsqueda de centroides}{equation.3.3.20}{}}
\abx@aux@cite{itml}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{Aprendiendo distancias para NCMC}{30}{section*.11}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.4}T\IeC {\'e}cnicas basadas en teor\IeC {\'\i }a de la informaci\IeC {\'o}n}{30}{section.3.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}ITML}{30}{subsection.3.4.1}}
\abx@aux@cite{dmlmj}
\newlabel{eq:itml:prob1}{{3.22}{31}{ITML}{equation.3.4.22}{}}
\newlabel{eq:itml:prob2}{{3.23}{31}{ITML}{equation.3.4.23}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}DMLMJ}{32}{subsection.3.4.2}}
\newlabel{eq:jef:thm1}{{3.24}{33}{}{equation.3.4.24}{}}
\newlabel{thm:dmlmj1}{{3.4.2}{33}{DMLMJ}{equation.3.4.24}{}}
\newlabel{eq:jef:formula_vp}{{3.25}{33}{DMLMJ}{equation.3.4.25}{}}
\newlabel{eq:jef:coro1_pre}{{3.26}{33}{DMLMJ}{equation.3.4.26}{}}
\newlabel{cor:dmlmj1}{{3.4.2}{33}{}{thm.3.4.2}{}}
\abx@aux@cite{mcml}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}MCML}{34}{subsection.3.4.3}}
\newlabel{eq:mcml:fobj2}{{3.30}{35}{MCML}{equation.3.4.30}{}}
\abx@aux@cite{lsi}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.5}Otras t\IeC {\'e}cnicas de aprendizaje de m\IeC {\'e}tricas de distancia}{36}{section.3.5}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}LSI}{36}{subsection.3.5.1}}
\newlabel{lsi}{{3.5.1}{36}{LSI}{subsection.3.5.1}{}}
\abx@aux@cite{dmleig}
\newlabel{eq:lsi}{{3.31}{37}{LSI}{equation.3.5.31}{}}
\newlabel{eq:lsi:equiv}{{3.32}{37}{LSI}{equation.3.5.32}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}DML-eig}{37}{subsection.3.5.2}}
\newlabel{eq:dmleig:1}{{3.33}{38}{DML-eig}{equation.3.5.33}{}}
\newlabel{eq:dmleig:2}{{3.34}{38}{DML-eig}{equation.3.5.34}{}}
\newlabel{eq:dmleig:3}{{3.35}{38}{}{equation.3.5.35}{}}
\newlabel{eq:dmleig:4}{{3.36}{38}{}{equation.3.5.36}{}}
\abx@aux@cite{overton1988minimizing}
\abx@aux@cite{ldml}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}LDML}{39}{subsection.3.5.3}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces La funci\IeC {\'o}n log\IeC {\'\i }stica.\relax }}{39}{figure.caption.12}}
\newlabel{fig:funcion_logistica}{{3.6}{39}{La función logística.\relax }{figure.caption.12}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.6}El kernel trick. Algoritmos de aprendizaje de m\IeC {\'e}tricas de distancia basados en kernels}{40}{section.3.6}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}El kernel trick}{40}{subsection.3.6.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Clasificaci\IeC {\'o}n realizada por las m\IeC {\'a}quinas de vectores soporte en su versi\IeC {\'o}n b\IeC {\'a}sica.\relax }}{41}{figure.caption.13}}
\newlabel{fig:svm_ejemplo}{{3.7}{41}{Clasificación realizada por las máquinas de vectores soporte en su versión básica.\relax }{figure.caption.13}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Conjunto de datos para el que SVM no puede establecer un hiperplano separador.\relax }}{41}{figure.caption.14}}
\newlabel{fig:svm_ejemplo2}{{3.8}{41}{Conjunto de datos para el que SVM no puede establecer un hiperplano separador.\relax }{figure.caption.14}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Resoluci\IeC {\'o}n mediante m\IeC {\'a}quinas de vectores soporte del problema de la figura \ref  {fig:svm_ejemplo2} en el espacio de caracter\IeC {\'\i }sticas. A la derecha se muestra el efecto del clasificador aprendido en el espacio de caracter\IeC {\'\i }sticas sobre el conjunto de datos original.\relax }}{42}{figure.caption.15}}
\newlabel{fig:svm_ejemplo3}{{3.9}{42}{Resolución mediante máquinas de vectores soporte del problema de la figura \ref {fig:svm_ejemplo2} en el espacio de características. A la derecha se muestra el efecto del clasificador aprendido en el espacio de características sobre el conjunto de datos original.\relax }{figure.caption.15}{}}
\newlabel{eq:dist_features}{{3.39}{44}{El kernel trick}{equation.3.6.39}{}}
\abx@aux@cite{klmnn}
\newlabel{thm:representer}{{3.6.5}{45}{Teorema de representación para el aprendizaje de métricas de distancia}{thm.3.6.5}{}}
\newlabel{item:representer:1}{{{{a)}}}{45}{Teorema de representación para el aprendizaje de métricas de distancia}{Item.4}{}}
\newlabel{item:representer:2}{{{{b)}}}{45}{Teorema de representación para el aprendizaje de métricas de distancia}{equation.3.6.40}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}KLMNN}{45}{subsection.3.6.2}}
\abx@aux@cite{anmm}
\newlabel{eq:klmnn:L}{{3.41}{46}{KLMNN}{equation.3.6.41}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.3}KANMM}{46}{subsection.3.6.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.4}KDMLMJ}{47}{subsection.3.6.4}}
\abx@aux@cite{kda}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.5}KDA}{48}{subsection.3.6.5}}
\newlabel{eq:kda}{{3.44}{48}{KDA}{equation.3.6.44}{}}
\newlabel{LastPage}{{}{51}{}{page.51}{}}
\xdef\lastpage@lastpage{51}
\xdef\lastpage@lastpageHy{51}
